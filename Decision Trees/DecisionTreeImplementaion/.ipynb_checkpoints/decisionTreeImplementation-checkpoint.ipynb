{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee79a70f",
   "metadata": {},
   "source": [
    "# 1. Printing Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6247fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "92235c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loading iris data set\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "18cb696a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing input X  in df (for visulaising data in table)\n",
    "df = pd.DataFrame(X)\n",
    "df.columns = iris.feature_names\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "95e47151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14220c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "edbee3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "61dcce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates entropy\n",
    "def entropy(Y):\n",
    "    \n",
    "    e = 0         #  stores entropy \n",
    "    total = len(Y)  #total len of Output Y\n",
    "    \n",
    "    classes = set(Y)  # stores all unique values in Y\n",
    "    \n",
    "    # iterating over classes\n",
    "    for c in classes:\n",
    "        count = len(Y[Y==c])  # total values in Y which are equal to class c\n",
    "        e += (count/total) * math.log2(count/total)  # applying entropy formula (pi * log(pi))\n",
    "    \n",
    "    return -e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "16e4870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gain function works for datasets containing continuous values (like :- iris dataset)\n",
    "# finds gain ratio and split point if splitted by fth feature\n",
    "def gain(X,Y,f): \n",
    "    \n",
    "    info_n = entropy(Y)  # stores entropy value before tree split\n",
    "    \n",
    "    points = X[:,f].copy();       # stores all points at fth column in X\n",
    "    \n",
    "    dic = {}                      # stores points as keys, and output of the row ,at which particular point lies, as value\n",
    "    for i in range(0,len(points)):\n",
    "        dic[points[i]] = Y[i]\n",
    "        \n",
    "    points.sort()                 # sorting the points of fth column\n",
    "    \n",
    "    reorder_Y = np.zeros(len(Y))  # it will store values present inside Y after reordering them according to sorted points\n",
    "    for i in range(len(points)): \n",
    "        reorder_Y[i] = dic[points[i]]\n",
    "        \n",
    "    gain=0             # used to store gain after split\n",
    "    split_point = None # used to store split_point\n",
    "    split_info  = None # used to store final split info\n",
    "    \n",
    "    gain_ratio = -38;\n",
    "    \n",
    "    #iterating over all points\n",
    "    for i in range(0,len(points)-1):\n",
    "    \n",
    "        curr_gain_ratio = 0\n",
    "        l = len(points)   # total points\n",
    "        \n",
    "        d1 = i+1          # total points in 1st half of split\n",
    "        d2 = l -d1        # total points in 2nd half of split\n",
    "        \n",
    "        #entropy if we split at mid-point of ith and (i+1)th point\n",
    "        info_f = ((d1/l)*entropy(reorder_Y[0:i+1])) + ((d2/l)*entropy(reorder_Y[i+1:])) \n",
    "        \n",
    "        curr_gain = info_n - info_f # stores gain after this split\n",
    "        \n",
    "        curr_split_info = -( ( (d1/l)*math.log2(d1/l) ) + ( (d2/l)*math.log2(d2/l) ) )\n",
    "        curr_gain_ratio = curr_gain/curr_split_info\n",
    "        \n",
    "        if(curr_gain>gain):\n",
    "            gain = curr_gain  # update gain\n",
    "            split_point = (points[i] + points[i+1])/2  #update split point\n",
    "            split_info = -( ( (d1/l)*math.log2(d1/l) ) + ( (d2/l)*math.log2(d2/l) ) ) # update split info\n",
    "            \n",
    "            \n",
    "    gain_ratio = gain/split_info  # calculate gain_ratio\n",
    "    \n",
    "    return (gain_ratio,split_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d324fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "936757e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print decision_tree function\n",
    "def decision_tree(X,Y,features,i):\n",
    "    \n",
    "\n",
    "    if len(set(Y))==1 or len(features)==0:  # if every input belongs to 1 class or there is no feature to split\n",
    "        print(\"Reached leaf Node\",\"\\n\")\n",
    "        return\n",
    "    \n",
    "    #printing current level of the tree\n",
    "    print(\"Level\",i)\n",
    "    print(\"Count of iris-sertosa =\",len(Y[Y==0]))\n",
    "    print(\"Count of versicolour =\",len(Y[Y==1]))\n",
    "    print(\"Count of virginica =\",len(Y[Y==2]))\n",
    "    print(\"Current Entropy is =\",entropy(Y))\n",
    "    \n",
    "    max_gain_ratio = -9999999      #used to store max gain   \n",
    "    selected_feature = None  #used to store index of the feature by which we'll split our tree\n",
    "    \n",
    "    split_point = -999999     # stores point used to split selected feature column values \n",
    "    \n",
    "    \n",
    "    # iterating on feature keys\n",
    "    for f in features.keys():\n",
    "        curr_gain_ratio,curr_split_point = gain(X,Y,features[f])  # returns gain and split point if split by fth feature of X\n",
    "        \n",
    "        if max_gain_ratio<curr_gain_ratio:                    # if true , store curr_gain, feature index j, and split point\n",
    "            max_gain_ratio=curr_gain_ratio\n",
    "            selected_feature = f\n",
    "            split_point = curr_split_point\n",
    "    \n",
    "    unique_values = set(X[:,features[selected_feature]])     # get all unique values from selected feature column in X\n",
    "    sel_feature_col = features[selected_feature]  # stores index of selected feature (used as column in input X)\n",
    "    del features[selected_feature]    # remove selected feature from features dictionary\n",
    "    \n",
    "    \n",
    "    print(\"Splitting the feature\",selected_feature,\"with gain ratio\",max_gain_ratio,\"\\n\")\n",
    "\n",
    "    # recursive calls\n",
    "    decision_tree(X[X[:,sel_feature_col]<split_point],Y[X[:,sel_feature_col]<split_point],features,i+1) \n",
    "    decision_tree(X[X[:,sel_feature_col]>=split_point],Y[X[:,sel_feature_col]>=split_point],features,i+1)\n",
    "    \n",
    "    features[selected_feature] = sel_feature_col   # inserting feature back to dic (it will be used by previous rec calls ,(recursion logic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1691dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "68b4fe2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of iris-sertosa = 50\n",
      "Count of versicolour = 50\n",
      "Count of virginica = 50\n",
      "Current Entropy is = 1.584962500721156\n",
      "Splitting the feature sepal width (cm) with gain ratio 1.7494806453264553 \n",
      "\n",
      "Level 1\n",
      "Count of iris-sertosa = 28\n",
      "Count of versicolour = 50\n",
      "Count of virginica = 47\n",
      "Current Entropy is = 1.542868897980502\n",
      "Splitting the feature petal width (cm) with gain ratio 1.1034940508701514 \n",
      "\n",
      "Level 2\n",
      "Count of iris-sertosa = 28\n",
      "Count of versicolour = 28\n",
      "Count of virginica = 0\n",
      "Current Entropy is = 1.0\n",
      "Splitting the feature petal length (cm) with gain ratio 1.0 \n",
      "\n",
      "Reached leaf Node \n",
      "\n",
      "Reached leaf Node \n",
      "\n",
      "Level 2\n",
      "Count of iris-sertosa = 0\n",
      "Count of versicolour = 22\n",
      "Count of virginica = 47\n",
      "Current Entropy is = 0.9031161721568719\n",
      "Splitting the feature sepal length (cm) with gain ratio 2.619185632333179 \n",
      "\n",
      "Level 3\n",
      "Count of iris-sertosa = 0\n",
      "Count of versicolour = 2\n",
      "Count of virginica = 1\n",
      "Current Entropy is = 0.9182958340544896\n",
      "Splitting the feature petal length (cm) with gain ratio 1.0 \n",
      "\n",
      "Reached leaf Node \n",
      "\n",
      "Reached leaf Node \n",
      "\n",
      "Level 3\n",
      "Count of iris-sertosa = 0\n",
      "Count of versicolour = 20\n",
      "Count of virginica = 46\n",
      "Current Entropy is = 0.8849636363831528\n",
      "Splitting the feature petal length (cm) with gain ratio 1.1870455481609554 \n",
      "\n",
      "Reached leaf Node \n",
      "\n",
      "Reached leaf Node \n",
      "\n",
      "Level 1\n",
      "Count of iris-sertosa = 22\n",
      "Count of versicolour = 0\n",
      "Count of virginica = 3\n",
      "Current Entropy is = 0.5293608652873644\n",
      "Splitting the feature petal length (cm) with gain ratio 1.0 \n",
      "\n",
      "Reached leaf Node \n",
      "\n",
      "Reached leaf Node \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#__________________________main code__________________________________\n",
    "\n",
    "features = iris.feature_names.copy()\n",
    "dic_features = {}  # it stores feature as key and feature index as value \n",
    "                  #(using this because in input X columns will be accessed by column index)\n",
    "    \n",
    "\n",
    "for i in range(len(features)):\n",
    "    dic_features[features[i]] = i\n",
    "    \n",
    "decision_tree(X,Y,dic_features,0)  # prints decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bb53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5a27d8f",
   "metadata": {},
   "source": [
    "# 2. Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f064e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _calculate_entropy(self, Y):\n",
    "\n",
    "        e = 0         #  stores entropy \n",
    "        total = len(Y)  #total len of Output Y\n",
    "\n",
    "        classes = set(Y)  # stores all unique values in Y\n",
    "\n",
    "        # iterating over classes\n",
    "        for c in classes:\n",
    "            count = len(Y[Y==c])  # total values in Y which are equal to class c\n",
    "            e += (count/total) * math.log2(count/total)  # applying entropy formula (pi * log(pi))\n",
    "\n",
    "        return -e\n",
    "    \n",
    "    \n",
    "    def _gain(self,X,Y,f): \n",
    "        \n",
    "    \n",
    "        info_n = self._calculate_entropy(Y)  # stores entropy value before tree split\n",
    "\n",
    "        points = X[:,f].copy();       # stores all points at fth column in X\n",
    "\n",
    "        dic = {}                      # stores points as keys, and output of the row ,at which particular point lies, as value\n",
    "        for i in range(0,len(points)):\n",
    "            dic[points[i]] = Y[i]\n",
    "\n",
    "        points.sort()                 # sorting the points of fth column\n",
    "\n",
    "        reorder_Y = np.zeros(len(Y))  # it will store values present inside Y after reordering them according to sorted points\n",
    "        for i in range(len(points)): \n",
    "            reorder_Y[i] = dic[points[i]]\n",
    "\n",
    "        gain=0             # used to store gain after split\n",
    "        split_point = None # used to store split_point\n",
    "        split_info  = None # used to store final split info\n",
    "\n",
    "        gain_ratio = -99999;\n",
    "\n",
    "        #iterating over all points\n",
    "        for i in range(0,len(points)-1):\n",
    "\n",
    "            curr_gain_ratio = 0\n",
    "            l = len(points)   # total points\n",
    "\n",
    "            d1 = i+1          # total points in 1st half of split\n",
    "            d2 = l -d1        # total points in 2nd half of split\n",
    "\n",
    "            #entropy if we split at mid-point of ith and (i+1)th point\n",
    "            info_f = ((d1/l)*self._calculate_entropy(reorder_Y[0:i+1])) + ((d2/l)*self._calculate_entropy(reorder_Y[i+1:])) \n",
    "\n",
    "            curr_gain = info_n - info_f # stores gain after this split\n",
    "\n",
    "            curr_split_info = -( ( (d1/l)*math.log2(d1/l) ) + ( (d2/l)*math.log2(d2/l) ) )\n",
    "            curr_gain_ratio = curr_gain/curr_split_info\n",
    "\n",
    "            if(curr_gain>gain):\n",
    "                gain = curr_gain  # update gain\n",
    "                split_point = (points[i] + points[i+1])/2  #update split point\n",
    "                split_info = -( ( (d1/l)*math.log2(d1/l) ) + ( (d2/l)*math.log2(d2/l) ) ) # update split info\n",
    "\n",
    "\n",
    "        gain_ratio = gain/split_info  # calculate gain_ratio\n",
    "\n",
    "        return (gain_ratio,split_point)\n",
    "\n",
    "\n",
    "    def _split_data(self, X, Y, feature_index, threshold):\n",
    "        # Split data based on a given feature and threshold\n",
    "        \n",
    "        X_left = X[X[:,feature_index]<threshold]\n",
    "        X_right = X[X[:,feature_index]>=threshold]\n",
    "        \n",
    "        Y_left = Y[X[:,feature_index]<threshold]\n",
    "        Y_right = Y[X[:,feature_index]>=threshold]\n",
    "\n",
    "        return X_left, Y_left, X_right, Y_right\n",
    "\n",
    "    def _build_tree(self, X, y,features, depth):\n",
    "        \n",
    "        # Recursively build the decision tree\n",
    "        if depth == 0 or len(set(y)) == 1 or len(features)==0:\n",
    "            # Stop building if max depth is reached or pure node is achieved\n",
    "            unique_classes, counts = np.unique(y, return_counts=True)\n",
    "            index_of_most_common = np.argmax(counts)\n",
    "            return {\"class\": unique_classes[index_of_most_common]}\n",
    "\n",
    "        max_gain_ratio=-99999\n",
    "        threshold=None\n",
    "        feature_index = None\n",
    "\n",
    "        for f in features.keys():\n",
    "            curr_gain_ratio,curr_split_point = self._gain(X,y,f)  # returns gain and split point if split by fth feature of X\n",
    "\n",
    "            if max_gain_ratio<curr_gain_ratio:                    # if true , store curr_gain, feature index j, and split point\n",
    "                max_gain_ratio=curr_gain_ratio\n",
    "                threshold = curr_split_point\n",
    "                feature_index = f\n",
    "\n",
    "\n",
    "        X_left, y_left, X_right, y_right = self._split_data(X, y, feature_index, threshold)\n",
    "        \n",
    "        \n",
    "        if depth!=None:    # if depth not specified while creating decision tree classifier , default value of depth will be none\n",
    "            depth -= 1\n",
    "            \n",
    "        feature_name = features[feature_index]   # points to feautre at which we will split at this level\n",
    "        del features[feature_index]            # removing feature used to split, (rest of the features will be passed to next recursive function)\n",
    "        \n",
    "        smallAns =  {                                      # small ans will be returned to previous recursive function\n",
    "            \"split_by_feature\":feature_name,               # data ...\n",
    "            \"iris_sertosa\": len(y[y==0]),\n",
    "            \"iris_versicolour\": len(y[y==1]),\n",
    "            \"iris_virginica\": len(y[y==2]),\n",
    "            \"current_entropy\": self._calculate_entropy(y),\n",
    "            \"gain_ratio\": max_gain_ratio,\n",
    "            \"threshold\": threshold,\n",
    "            \"left\": self._build_tree(X_left, y_left, features,depth),  # 1st half of curr split (recursive call)\n",
    "            \"right\": self._build_tree(X_right, y_right, features,depth) # 2nd half of curr split (recursive call)\n",
    "        }\n",
    "        \n",
    "        features[feature_index] = feature_name  # inserting splitted feature back to features (will be used by prev recursive functions)\n",
    "        \n",
    "        return smallAns\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        features = {0:'sepal length (cm)',1:'sepal width (cm)',2:'petal length (cm)',3:'petal width (cm)'} # features of iris dataset\n",
    "        self.tree = self._build_tree(X, y,features, self.max_depth) #building decision tree\n",
    "\n",
    "    def _predict_instance(self, instance, node):\n",
    "        if \"class\" in node:         # condition to handle leaf node of decision tree\n",
    "            return node[\"class\"]\n",
    "        if instance[node[\"feature_index\"]] <= node[\"threshold\"]:   # if true calling left half\n",
    "            return self._predict_instance(instance, node[\"left\"])\n",
    "        else:                                                       # else calling right half\n",
    "            return self._predict_instance(instance, node[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_instance(instance, self.tree) for instance in X]  # iterates over each row in X and predicts output\n",
    "    \n",
    "    def _print_tree_recursive(self, node, level=0): # recursively prints tree (according to data points used to train)\n",
    "        \n",
    "        if \"class\" in node:       # returns if reached leaf node            \n",
    "            print(\"Reached leaf node\")\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # printing details of current level\n",
    "        print(\"level\",level) \n",
    "        print(\"Count of iris-sertosa =\",node[\"iris_sertosa\"])\n",
    "        print(\"Count of versicolour =\",node[\"iris_versicolour\"])\n",
    "        print(\"Count of virginica =\",node[\"iris_virginica\"])\n",
    "        print(\"Current Entropy is =\",node[\"current_entropy\"])\n",
    "        print(\"splitting on feature\",node[\"split_by_feature\"],\"with gain ratio\",node[\"gain_ratio\"],\"\\n\")\n",
    "        \n",
    "        \n",
    "        # making recursive calls\n",
    "        self._print_tree_recursive(node[\"left\"],level+1)\n",
    "        self._print_tree_recursive(node[\"right\"],level+1)\n",
    "        \n",
    "       \n",
    "\n",
    "    def print_tree(self):\n",
    "        if self.tree is not None:    # prints decision tree recursively if decisison tree is fitted \n",
    "            print(\"\\nDecision Tree Structure:\\n\")\n",
    "            self._print_tree_recursive(self.tree)\n",
    "            \n",
    "        else:            # if decision tree model is not trained yet\n",
    "            print(\"Decision tree not fitted yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07921153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8c459d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = DecisionTree()  # creating instance of Decision Tree class\n",
    "clf.fit(X,Y)  # fitting input points X  and output Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4c5e309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Structure:\n",
      "\n",
      "level 0\n",
      "Count of iris-sertosa = 50\n",
      "Count of versicolour = 50\n",
      "Count of virginica = 50\n",
      "Current Entropy is = 1.584962500721156\n",
      "splitting on feature sepal width (cm) with gain ratio 1.7494806453264553 \n",
      "\n",
      "level 1\n",
      "Count of iris-sertosa = 28\n",
      "Count of versicolour = 50\n",
      "Count of virginica = 47\n",
      "Current Entropy is = 1.542868897980502\n",
      "splitting on feature petal width (cm) with gain ratio 1.1034940508701514 \n",
      "\n",
      "level 2\n",
      "Count of iris-sertosa = 28\n",
      "Count of versicolour = 28\n",
      "Count of virginica = 0\n",
      "Current Entropy is = 1.0\n",
      "splitting on feature petal length (cm) with gain ratio 1.0 \n",
      "\n",
      "Reached leaf node\n",
      "Reached leaf node\n",
      "level 2\n",
      "Count of iris-sertosa = 0\n",
      "Count of versicolour = 22\n",
      "Count of virginica = 47\n",
      "Current Entropy is = 0.9031161721568719\n",
      "splitting on feature sepal length (cm) with gain ratio 2.619185632333179 \n",
      "\n",
      "level 3\n",
      "Count of iris-sertosa = 0\n",
      "Count of versicolour = 2\n",
      "Count of virginica = 1\n",
      "Current Entropy is = 0.9182958340544896\n",
      "splitting on feature petal length (cm) with gain ratio 1.0 \n",
      "\n",
      "Reached leaf node\n",
      "Reached leaf node\n",
      "level 3\n",
      "Count of iris-sertosa = 0\n",
      "Count of versicolour = 20\n",
      "Count of virginica = 46\n",
      "Current Entropy is = 0.8849636363831528\n",
      "splitting on feature petal length (cm) with gain ratio 1.1870455481609554 \n",
      "\n",
      "Reached leaf node\n",
      "Reached leaf node\n",
      "level 1\n",
      "Count of iris-sertosa = 22\n",
      "Count of versicolour = 0\n",
      "Count of virginica = 3\n",
      "Current Entropy is = 0.5293608652873644\n",
      "splitting on feature petal length (cm) with gain ratio 1.0 \n",
      "\n",
      "Reached leaf node\n",
      "Reached leaf node\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf.print_tree() # printing structure of trained decision tree (structure depends of data points used to train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
